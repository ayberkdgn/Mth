{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74163843",
   "metadata": {},
   "source": [
    "Bu PDF ÅŸunu anlatÄ±r:\n",
    "\n",
    "> **Model eÄŸitildiâ€¦ peki gerÃ§ek hayatta binlerce kullanÄ±cÄ± aynÄ± anda soru sorunca ne oluyor?**\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## **Inference on LLM (LLMâ€™lerde Inference Nedir?)**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 1. Inference Nedir? (En Temel TanÄ±m)\n",
    "\n",
    "**Inference**, eÄŸitilmiÅŸ bir modelin **cevap Ã¼retme sÃ¼recidir**.\n",
    "\n",
    "Yani:\n",
    "\n",
    "* Training â†’ model Ã¶ÄŸrenir\n",
    "* **Inference â†’ model cevap verir**\n",
    "\n",
    "ğŸ“Œ GÃ¼nlÃ¼k hayatta:\n",
    "\n",
    "* ChatGPTâ€™ye soru soruyorsun\n",
    "* O an model **inference** yapÄ±yor\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 2. Training vs Inference ArasÄ±ndaki BÃ¼yÃ¼k Fark\n",
    "\n",
    "### ğŸŸ¢ Training (EÄŸitim)\n",
    "\n",
    "* Veri hazÄ±r\n",
    "* Batchâ€™ler kontrol altÄ±nda\n",
    "* GPU genelde boÅŸ kalmaz\n",
    "* Zaman kritik deÄŸildir\n",
    "\n",
    "### ğŸ”´ Inference (Servis)\n",
    "\n",
    "* Ä°stekler **rastgele gelir**\n",
    "* Her isteÄŸin uzunluÄŸu farklÄ±dÄ±r\n",
    "* Gecikme (latency) Ã§ok kritiktir\n",
    "* GPU boÅŸa Ã§alÄ±ÅŸmamalÄ±dÄ±r\n",
    "\n",
    "ğŸ“Œ Bu PDFâ€™in odaÄŸÄ±: **Inference problemleri**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 3. Transformerâ€™da Inference Problemi Nereden Geliyor?\n",
    "\n",
    "Bir transformer ÅŸunu varsayar:\n",
    "\n",
    "* Batchâ€™teki tÃ¼m inputâ€™lar **aynÄ± uzunlukta**\n",
    "\n",
    "Ama gerÃ§ek hayatta:\n",
    "\n",
    "* KullanÄ±cÄ± A: 5 token\n",
    "* KullanÄ±cÄ± B: 300 token\n",
    "* KullanÄ±cÄ± C: 50 token\n",
    "\n",
    "ğŸ“Œ Ä°ÅŸte bÃ¼tÃ¼n karmaÅŸa buradan Ã§Ä±kar.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 4. Output Ãœretimi (Autoregressive Generation)\n",
    "\n",
    "LLMâ€™ler cevabÄ± **tek seferde Ã¼retmez**.\n",
    "\n",
    "Åu ÅŸekilde Ã§alÄ±ÅŸÄ±r:\n",
    "\n",
    "1. Ä°lk tokenâ€™Ä± Ã¼ret\n",
    "2. Onu inputâ€™a ekle\n",
    "3. Ä°kinci tokenâ€™Ä± Ã¼ret\n",
    "4. BÃ¶yle bÃ¶yle devam et\n",
    "\n",
    "ğŸ“Œ Buna **autoregressive generation** denir.\n",
    "\n",
    "SonuÃ§:\n",
    "\n",
    "* Her token = yeni hesaplama\n",
    "* Uzun cevap = daha fazla sÃ¼re\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 5. Sampling KavramlarÄ± (Cevap NasÄ±l SeÃ§iliyor?)\n",
    "\n",
    "Model her adÄ±mda:\n",
    "\n",
    "* Bir sÃ¼rÃ¼ kelime iÃ§in olasÄ±lÄ±k Ã¼retir\n",
    "\n",
    "Ama hepsini kullanmaz.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¸ Temperature Nedir?\n",
    "\n",
    "* Rastgelelik kontrolÃ¼\n",
    "\n",
    "DÃ¼ÅŸÃ¼k temperature:\n",
    "\n",
    "* Daha kesin\n",
    "* Daha sÄ±kÄ±cÄ±\n",
    "\n",
    "YÃ¼ksek temperature:\n",
    "\n",
    "* Daha yaratÄ±cÄ±\n",
    "* Daha riskli\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¸ Top-k\n",
    "\n",
    "* En yÃ¼ksek olasÄ±lÄ±klÄ± **k** kelimeyi tut\n",
    "* Gerisini sil\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¸ Top-p (Nucleus Sampling)\n",
    "\n",
    "* Toplam olasÄ±lÄ±ÄŸÄ± %pâ€™ye ulaÅŸana kadar kelimeleri tut\n",
    "\n",
    "ğŸ“Œ SÄ±nav cÃ¼mlesi:\n",
    "\n",
    "> Temperature, top-k ve top-p, LLMâ€™in cevap Ã§eÅŸitliliÄŸini kontrol eder.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 6. Parallelism Problemi (Model GPUâ€™ya SÄ±ÄŸmazsa?)\n",
    "\n",
    "Soru:\n",
    "\n",
    "> Model tek GPUâ€™ya sÄ±ÄŸmazsa ne yaparÄ±z?\n",
    "\n",
    "Cevaplar:\n",
    "\n",
    "1. Daha kÃ¼Ã§Ã¼k model\n",
    "2. Quantization\n",
    "3. **Modeli GPUâ€™lara bÃ¶l**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 7. Tensor Parallelism Nedir?\n",
    "\n",
    "**Tensor Parallelism**:\n",
    "\n",
    "* AÄŸÄ±rlÄ±klarÄ± bÃ¶l\n",
    "* AynÄ± layer farklÄ± GPUâ€™larda Ã§alÄ±ÅŸÄ±r\n",
    "\n",
    "ğŸ“Œ ArtÄ±:\n",
    "\n",
    "* BÃ¼yÃ¼k modeller Ã§alÄ±ÅŸÄ±r\n",
    "\n",
    "ğŸ“Œ Eksi:\n",
    "\n",
    "* SÃ¼rekli GPU arasÄ± veri transferi\n",
    "* Nodeâ€™lar arasÄ± yavaÅŸ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 8. Pipeline Parallelism Nedir?\n",
    "\n",
    "**Pipeline Parallelism**:\n",
    "\n",
    "* Layerâ€™larÄ± bÃ¶l\n",
    "* Her GPU farklÄ± katmanÄ± Ã§alÄ±ÅŸtÄ±rÄ±r\n",
    "\n",
    "ğŸ“Œ ArtÄ±:\n",
    "\n",
    "* Daha az veri transferi\n",
    "\n",
    "ğŸ“Œ Eksi:\n",
    "\n",
    "* Pipeline bubbles (boÅŸ bekleme)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 9. Tensor vs Pipeline (KarÅŸÄ±laÅŸtÄ±rma)\n",
    "\n",
    "| Tensor          | Pipeline       |\n",
    "| --------------- | -------------- |\n",
    "| AÄŸÄ±rlÄ±k bÃ¶lÃ¼nÃ¼r | Katman bÃ¶lÃ¼nÃ¼r |\n",
    "| Ä°letiÅŸim fazla  | Ä°letiÅŸim az    |\n",
    "| Daha karmaÅŸÄ±k   | Daha basit     |\n",
    "\n",
    "ğŸ“Œ SÄ±navda tablo sorusu Ã§Ä±kabilir.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 10. Batching Nedir?\n",
    "\n",
    "**Batching**:\n",
    "\n",
    "> Birden fazla isteÄŸi aynÄ± anda modele sokmak\n",
    "\n",
    "AmaÃ§:\n",
    "\n",
    "* GPUâ€™yu daha verimli kullanmak\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 11. Static Batching (Neden KÃ¶tÃ¼?)\n",
    "\n",
    "Static batchingâ€™de:\n",
    "\n",
    "* Batch sabit\n",
    "* Herkes birbirini bekler\n",
    "\n",
    "Sorunlar:\n",
    "\n",
    "* KÄ±sa istekler boÅŸ yere hesaplanÄ±r\n",
    "* Uzun istekler herkesi bekletir\n",
    "* Yeni gelen istek bekler\n",
    "\n",
    "ğŸ“Œ SonuÃ§:\n",
    "\n",
    "* YÃ¼ksek latency\n",
    "* GPU verimsiz\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 12. Continuous (Iterative) Batching\n",
    "\n",
    "Bu problemi Ã§Ã¶zmek iÃ§in:\n",
    "\n",
    "* **ORCA** yaklaÅŸÄ±mÄ±\n",
    "\n",
    "MantÄ±k:\n",
    "\n",
    "* Bitene â€œÃ§Ä±kâ€ denir\n",
    "* Yeni gelen hemen batchâ€™e girer\n",
    "\n",
    "ğŸ“Œ SonuÃ§:\n",
    "\n",
    "* GPU dolu Ã§alÄ±ÅŸÄ±r\n",
    "* Daha dÃ¼ÅŸÃ¼k latency\n",
    "* Daha yÃ¼ksek throughput\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 13. Prefill vs Decode (Ã‡OK Ã–NEMLÄ°)\n",
    "\n",
    "### ğŸ”¸ Prefill\n",
    "\n",
    "* Ä°lk token Ã¼retimi\n",
    "* Compute bound\n",
    "* GPUâ€™yu Ã§ok kullanÄ±r\n",
    "\n",
    "### ğŸ”¸ Decode\n",
    "\n",
    "* Sonraki tokenâ€™lar\n",
    "* Memory bound\n",
    "* KV-cache aÄŸÄ±rlÄ±klÄ±\n",
    "\n",
    "ğŸ“Œ Scheduler burada karar verir:\n",
    "\n",
    "> â€œHangisini Ã¶nce Ã§alÄ±ÅŸtÄ±rayÄ±m?â€\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 14. Attention OptimizasyonlarÄ±\n",
    "\n",
    "### ğŸ”¸ Flash Attention\n",
    "\n",
    "* Prefillâ€™i hÄ±zlandÄ±rÄ±r\n",
    "* Bellek eriÅŸimini optimize eder\n",
    "\n",
    "### ğŸ”¸ Page Attention\n",
    "\n",
    "* KV-cacheâ€™i sayfalara bÃ¶ler\n",
    "* RAM gibi yÃ¶netir\n",
    "\n",
    "ğŸ“Œ Bunlar **yeni attention deÄŸil**, implementasyon tekniÄŸi.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 15. FarklÄ± Uzunluk Problemi\n",
    "\n",
    "### âŒ Padding\n",
    "\n",
    "* Gereksiz hesaplama\n",
    "* Latency artar\n",
    "\n",
    "### âœ… Ragged / 1D Batching\n",
    "\n",
    "* GerÃ§ek token sayÄ±sÄ± kadar iÅŸlem\n",
    "* Attention ayrÄ± ayrÄ± uygulanÄ±r\n",
    "\n",
    "ğŸ“Œ Modern inference motorlarÄ± bunu kullanÄ±r.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 16. Bu PDFâ€™in Ana MesajÄ±\n",
    "\n",
    "* Inference trainingâ€™den Ã§ok daha zordur\n",
    "* Latency & throughput kritik\n",
    "* Batching, parallelism ve attention optimizasyonu ÅŸarttÄ±r\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
