{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7993bfb5",
   "metadata": {},
   "source": [
    "# ğŸ§  Transformer & Attention â€” SÄ±fÄ±rdan + PDF Destekli SÃ¼per AnlatÄ±m\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Transformerâ€™Ä±n DoÄŸuÅŸu: Problem Ne?\n",
    "\n",
    "Dildeki en bÃ¼yÃ¼k zorluk, **uzun mesafe baÄŸÄ±mlÄ±lÄ±klarÄ±** anlamaktÄ±r.\n",
    "\n",
    "Ã–rneÄŸin:\n",
    "\n",
    "> â€œDÃ¼n kÃ¶peÄŸini kaybeden adamÄ± gÃ¶rdÃ¼m.â€\n",
    "\n",
    "Bu cÃ¼mlede:\n",
    "\n",
    "- â€œadamâ€ ve â€œkaybedenâ€ Ã§ok iliÅŸkili  \n",
    "- ama araya kelimeler giriyor\n",
    "\n",
    "Eski RNN / LSTM modelleri bu iliÅŸkileri yakalamakta zorlanÄ±yordu.\n",
    "\n",
    "**Transformer bunu Ã§Ã¶zdÃ¼**, Ã§Ã¼nkÃ¼ cÃ¼mledeki tÃ¼m kelimelere **aynÄ± anda** bakÄ±yor.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Transformerâ€™Ä±n Kalbi: ATTENTION (Dikkat MekanizmasÄ±)\n",
    "\n",
    "PDFâ€™de birÃ§ok sayfa â€œAttentionâ€ baÅŸlÄ±ÄŸÄ± altÄ±nda gÃ¶rsellerle anlatÄ±lmÄ±ÅŸ.\n",
    "\n",
    "Ancak PDF Ã§ok matematiÄŸe girmeden ÅŸunu sÃ¶ylÃ¼yor:\n",
    "\n",
    "- Attention, â€œtokenlar arasÄ±ndaki iliÅŸkileri paralel bulurâ€\n",
    "- Maliyet yÃ¼ksektir: **O(NÂ² Â· d)**  \n",
    "  â†’ N = token sayÄ±sÄ±, d = embedding boyutu\n",
    "\n",
    "Ama bunun mantÄ±ÄŸÄ± nedir?\n",
    "\n",
    "### ğŸ¯ MantÄ±ÄŸÄ±:\n",
    "\n",
    "Attention ÅŸu soruyu sorar:\n",
    "\n",
    "> â€œBen bir kelimeyi iÅŸlerken, hangi diÄŸer kelimelere ne kadar dikkat etmeliyim?â€\n",
    "\n",
    "Bunun iÃ§in Ã¼Ã§ vektÃ¶r oluÅŸturulur:\n",
    "\n",
    "- **Query (Q):** Sorgulayan kelime  \n",
    "- **Key (K):** DiÄŸer kelimelerin kimliÄŸi  \n",
    "- **Value (V):** Anlam / iÃ§erik\n",
    "\n",
    "Skor = Query Â· Key  \n",
    "â†’ Skor yÃ¼ksekse, o kelimenin Valueâ€™si daha Ã§ok alÄ±nÄ±r.\n",
    "\n",
    "Yani matematiksel olarak â€œdikkatiâ€ hesaplÄ±yoruz.\n",
    "\n",
    "Bu mekanizma PDFâ€™de gÃ¶rsellerle gÃ¶steriliyor.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Self-Attention: Her Token, Herkese Bakar\n",
    "\n",
    "Transformerâ€™daki en kritik ÅŸey: **Her kelime tÃ¼m diÄŸerlerine bakÄ±yor.**\n",
    "\n",
    "Bu yÃ¼zden uzun mesafeli iliÅŸkiler _mÃ¼kemmel_ yakalanÄ±yor.\n",
    "\n",
    "### Ã–rnek:\n",
    "\n",
    "â€œcat sat on the matâ€\n",
    "\n",
    "â€œsatâ€ kelimesi:\n",
    "\n",
    "- â€œcatâ€ ile Ã§ok iliÅŸkili â†’ yÃ¼ksek dikkat\n",
    "- â€œmatâ€ ile biraz iliÅŸkili\n",
    "- â€œtheâ€ ile Ã§ok az iliÅŸkili\n",
    "\n",
    "Bu hesaplamalar paralel yapÄ±lÄ±r.\n",
    "\n",
    "PDF'deki â€œAttentionâ€ gÃ¶rselleri bu iliÅŸkilerin nasÄ±l hesaplandÄ±ÄŸÄ±nÄ± gÃ¶steriyor.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Multi-Head Attention (MHA)\n",
    "\n",
    "PDFâ€™de bÃ¼yÃ¼k bir bÃ¶lÃ¼m Multi-Head Attentionâ€™a ayrÄ±lmÄ±ÅŸ.\n",
    "\n",
    "AnlamasÄ± kolay:\n",
    "\n",
    "Her Attention head **dilin baÅŸka bir boyutunu Ã¶ÄŸrenir**.\n",
    "\n",
    "- Bir head â†’ Ã¶zne-yÃ¼klem iliÅŸkisini Ã¶ÄŸrenebilir  \n",
    "- Bir head â†’ zaman iliÅŸkilerini Ã¶ÄŸrenebilir  \n",
    "- Bir head â†’ duygusal baÄŸlamÄ± Ã¶ÄŸrenebilir\n",
    "\n",
    "Her head farklÄ± bir bakÄ±ÅŸ aÃ§Ä±sÄ±dÄ±r.\n",
    "\n",
    "Sonra tÃ¼m headâ€™ler birleÅŸtirilir.\n",
    "\n",
    "Bu yÃ¼zden Transformer Ã§ok gÃ¼Ã§lÃ¼dÃ¼r.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Ama: Attention Pozisyon Bilmez\n",
    "\n",
    "Attention yalnÄ±zca iliÅŸkilere bakar; sÄ±rayÄ± bilmez.\n",
    "\n",
    "PDF de bunu net sÃ¶ylÃ¼yor:\n",
    "\n",
    "> â€œAttention does not consider the position information â€¦ positional encoding solutionâ€\n",
    "\n",
    "Bu yÃ¼zden Transformerâ€™a ekstra **Positional Encoding** eklenir.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Positional Encoding (PE)\n",
    "\n",
    "Transformer sÄ±rayÄ± bilmez, bu yÃ¼zden her token embeddingâ€™ine ek bir â€œpozisyon bilgisiâ€ eklenir.\n",
    "\n",
    "PDFâ€™de:\n",
    "\n",
    "- PEâ€™nin embedding ile toplandÄ±ÄŸÄ± gÃ¶steriliyor  \n",
    "- Modern modellerde RoPE (Rotary Position Embedding) kullanÄ±lÄ±yor\n",
    "\n",
    "MantÄ±ÄŸÄ±:  \n",
    "Her kelimenin embeddingâ€™ine, bulunduÄŸu konumu anlatan sinÃ¼sâ€“cosinÃ¼s dalgalarÄ± eklenir.\n",
    "\n",
    "SonuÃ§:  \n",
    "Transformere sÄ±ranÄ±n anlamÄ±nÄ± Ã¶ÄŸretmiÅŸ oluruz.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Normalization\n",
    "\n",
    "PDFâ€™de LayerNorm / RMSNorm farklarÄ± anlatÄ±lÄ±yor.\n",
    "\n",
    "Neden normalizasyon var?\n",
    "\n",
    "Ã‡Ã¼nkÃ¼:\n",
    "\n",
    "- Katmanlar bÃ¼yÃ¼dÃ¼kÃ§e deÄŸerler patlayabilir  \n",
    "- Ya da Ã§ok kÃ¼Ã§Ã¼lÃ¼p kaybolabilir\n",
    "\n",
    "Normalization:\n",
    "\n",
    "- AÄŸÄ±rlÄ±klarÄ± dengeler  \n",
    "- EÄŸitim stabil olur  \n",
    "- Daha hÄ±zlÄ± convergence saÄŸlar\n",
    "\n",
    "GPT-4 / Llama gibi modeller RMSNorm kullanÄ±r.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Feed Forward Network (FFN)\n",
    "\n",
    "PDFâ€™de Encoder/Decoder ÅŸemasÄ±nda gÃ¶rÃ¼nÃ¼yor.\n",
    "\n",
    "Transformerâ€™daki FFN aslÄ±nda ÅŸudur:\n",
    "\n",
    "> â€œAttention Ã§Ä±ktÄ±larÄ±nÄ± daha anlamlÄ± bir vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼ren kÃ¼Ã§Ã¼k bir MLP.â€\n",
    "\n",
    "- Her token baÄŸÄ±msÄ±z olarak iÅŸlenir  \n",
    "- Non-linearity eklenir  \n",
    "- Modelin daha kompleks iliÅŸkileri Ã¶ÄŸrenmesini saÄŸlar\n",
    "\n",
    "Activation olarak genelde **GELU** veya **SwiGLU** kullanÄ±lÄ±r.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Transformerâ€™Ä±n Genel Mimarisi (PDF ile uyumlu)\n",
    "\n",
    "PDFâ€™de Encoderâ€“Decoder ÅŸemasÄ± var.\n",
    "\n",
    "Ama modern modeller genelde decoder-only (GPT gibi).\n",
    "\n",
    "### Encoder-Decoder (T5, BART, MT modelleri)\n",
    "\n",
    "- Encoder: inputâ€™u anlamlandÄ±rÄ±r  \n",
    "- Decoder: output Ã¼retir  \n",
    "- Encoderâ€“decoder attention vardÄ±r  \n",
    "\n",
    "### Decoder-only (GPT, Llama)\n",
    "\n",
    "- YalnÄ±zca decoder bloÄŸu  \n",
    "- Masked self-attention kullanÄ±r  \n",
    "- Otoregresif Ã¼retim yapar\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Token NasÄ±l Ãœretiliyor? (PDFâ€™de detaylÄ± anlatÄ±m var)\n",
    "\n",
    "1. Her token iÃ§in model bÃ¼yÃ¼k bir vektÃ¶r (D boyutunda) Ã¼retir  \n",
    "2. Bu vektÃ¶r â€œdictionary sizeâ€ boyutlu logitsâ€™e Ã§evrilir  \n",
    "3. Softmax uygulanÄ±r  \n",
    "4. En yÃ¼ksek olasÄ±lÄ±k veya sampling yÃ¶ntemleriyle seÃ§im yapÄ±lÄ±r:\n",
    "\n",
    "   - **Top-k**  \n",
    "   - **Top-p**  \n",
    "   - **Temperature**\n",
    "\n",
    "Bu seÃ§im sÃ¼reci modelin â€œyaratÄ±cÄ±lÄ±ÄŸÄ±nÄ±â€ belirler.\n",
    "\n",
    "---\n",
    "\n",
    "## 11) KV-Cache (PDFâ€™de uzun anlatÄ±m)\n",
    "\n",
    "PDF bunu Ã§ok net aÃ§Ä±klÄ±yor:\n",
    "\n",
    "> â€œHer yeni token Ã¼retiminde iliÅŸkilerin baÅŸtan hesaplanmasÄ± maliyetlidirâ€¦ KV-cache Ã§Ã¶zÃ¼mÃ¼â€\n",
    "\n",
    "MantÄ±ÄŸÄ±:\n",
    "\n",
    "Normalde:\n",
    "\n",
    "- Her adÄ±mda tÃ¼m attention yeniden yapÄ±lÄ±r â†’ Ã§ok yavaÅŸ\n",
    "\n",
    "KV-cache:\n",
    "\n",
    "- Eski tokenlarÄ±n K ve V deÄŸerleri saklanÄ±r  \n",
    "- Sadece yeni token iÃ§in hesap yapÄ±lÄ±r  \n",
    "- Muazzam hÄ±z kazancÄ± saÄŸlar\n",
    "\n",
    "Dezavantaj:\n",
    "\n",
    "- HafÄ±za kullanÄ±mÄ± artar  \n",
    "- Decode kÄ±smÄ± hÄ±zlÄ± ama memory yoÄŸun\n",
    "\n",
    "---\n",
    "\n",
    "## 12) MQA & MHA & Grouped Attention (PDFâ€™de tablolar var)\n",
    "\n",
    "PDF bu Ã¼Ã§ yapÄ±yÄ± karÅŸÄ±laÅŸtÄ±rÄ±yor.\n",
    "\n",
    "### MHA = Multi Head Attention\n",
    "\n",
    "- Her head kendi Q,K,V matrislerine sahiptir  \n",
    "- Ã‡ok gÃ¼Ã§lÃ¼ ama maliyetli\n",
    "\n",
    "### MQA = Multi Query Attention\n",
    "\n",
    "- Q Ã§oklu  \n",
    "- K ve V *tek*  \n",
    "- Memory kullanÄ±mÄ± Ã§ok azalÄ±r  \n",
    "- Modern LLMâ€™ler MQA kullanÄ±r\n",
    "\n",
    "### GQA = Grouped Query Attention\n",
    "\n",
    "- Q headâ€™leri gruplara ayrÄ±lÄ±r  \n",
    "- MHA kadar gÃ¼Ã§lÃ¼ â†’ MQA kadar ucuz  \n",
    "- Llama 3 gibi modellerde kullanÄ±lÄ±r\n",
    "\n",
    "---\n",
    "\n",
    "## 13) MLA â€“ Multi-Head Latent Attention\n",
    "\n",
    "PDFâ€™de DeepSeek mimarisinden alÄ±ntÄ± var.\n",
    "\n",
    "MantÄ±ÄŸÄ±:\n",
    "\n",
    "- Q,K,V boyutlarÄ± normal MHAâ€™dan farklÄ± bir latent alana projeleniyor  \n",
    "- Daha kompakt ama etkili bir dikkat mekanizmasÄ±\n",
    "\n",
    "Yeni nesil modellerde optimize bir alternatif.\n",
    "\n",
    "---\n",
    "\n",
    "## 14) MoE â€“ Mixture of Experts\n",
    "\n",
    "PDFâ€™nin sonunda bahsedilen en ileri konu.\n",
    "\n",
    "MoE ÅŸu fikre dayanÄ±r:\n",
    "\n",
    "> â€œHer token iÃ§in tÃ¼m modeli Ã§alÄ±ÅŸtÄ±rmaya gerek yok.  \n",
    ">  BazÄ± uzmanlar (experts) belirli tÃ¼r verilerde daha iyidir.â€\n",
    "\n",
    "Bu yÃ¼zden:\n",
    "\n",
    "- Ã‡ok sayÄ±da uzman FFN vardÄ±r  \n",
    "- Router, hangi tokenÄ±n hangi uzmanlara gitmesi gerektiÄŸini belirler  \n",
    "- Sadece %5â€“10 uzman aktif olur â†’ Ã§ok bÃ¼yÃ¼k modeller dÃ¼ÅŸÃ¼k hesaplama ile Ã§alÄ±ÅŸÄ±r\n",
    "\n",
    "Avantaj:\n",
    "\n",
    "- Parametre sayÄ±sÄ± devasa olabilir ama hesaplama kÃ¼Ã§Ã¼k kalÄ±r  \n",
    "- Verimlilik Ã§ok artar\n",
    "\n",
    "Dezavantaj:\n",
    "\n",
    "- EÄŸitim zordur  \n",
    "- Load balancing gerektirir  \n",
    "- Router karmaÅŸÄ±ktÄ±r\n",
    "\n",
    "DeepSeek, Gemini, Qwen3 gibi modern LLMâ€™ler MoE kullanÄ±r.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
