{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7993bfb5",
   "metadata": {},
   "source": [
    "# ğŸ§  Transformer & Attention â€” SÄ±fÄ±rdan + PDF Destekli SÃ¼per AnlatÄ±m\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Transformerâ€™Ä±n DoÄŸuÅŸu: Problem Ne?\n",
    "\n",
    "Dildeki en bÃ¼yÃ¼k zorluk, **uzun mesafe baÄŸÄ±mlÄ±lÄ±klarÄ±** anlamaktÄ±r.\n",
    "\n",
    "Ã–rneÄŸin:\n",
    "\n",
    "> â€œDÃ¼n kÃ¶peÄŸini kaybeden adamÄ± gÃ¶rdÃ¼m.â€\n",
    "\n",
    "Bu cÃ¼mlede:\n",
    "\n",
    "- â€œadamâ€ ve â€œkaybedenâ€ Ã§ok iliÅŸkili  \n",
    "- ama araya kelimeler giriyor\n",
    "\n",
    "Eski RNN / LSTM modelleri bu iliÅŸkileri yakalamakta zorlanÄ±yordu.\n",
    "\n",
    "**Transformer bunu Ã§Ã¶zdÃ¼**, Ã§Ã¼nkÃ¼ cÃ¼mledeki tÃ¼m kelimelere **aynÄ± anda** bakÄ±yor.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Transformerâ€™Ä±n Kalbi: ATTENTION (Dikkat MekanizmasÄ±)\n",
    "\n",
    "PDFâ€™de birÃ§ok sayfa â€œAttentionâ€ baÅŸlÄ±ÄŸÄ± altÄ±nda gÃ¶rsellerle anlatÄ±lmÄ±ÅŸ.\n",
    "\n",
    "Ancak PDF Ã§ok matematiÄŸe girmeden ÅŸunu sÃ¶ylÃ¼yor:\n",
    "\n",
    "- Attention, â€œtokenlar arasÄ±ndaki iliÅŸkileri paralel bulurâ€\n",
    "- Maliyet yÃ¼ksektir: **O(NÂ² Â· d)**  \n",
    "  â†’ N = token sayÄ±sÄ±, d = embedding boyutu\n",
    "\n",
    "Ama bunun mantÄ±ÄŸÄ± nedir?\n",
    "\n",
    "### ğŸ¯ MantÄ±ÄŸÄ±:\n",
    "\n",
    "Attention ÅŸu soruyu sorar:\n",
    "\n",
    "> â€œBen bir kelimeyi iÅŸlerken, hangi diÄŸer kelimelere ne kadar dikkat etmeliyim?â€\n",
    "\n",
    "Bunun iÃ§in Ã¼Ã§ vektÃ¶r oluÅŸturulur:\n",
    "\n",
    "- **Query (Q):** Sorgulayan kelime  \n",
    "- **Key (K):** DiÄŸer kelimelerin kimliÄŸi  \n",
    "- **Value (V):** Anlam / iÃ§erik\n",
    "\n",
    "Skor = Query Â· Key  \n",
    "â†’ Skor yÃ¼ksekse, o kelimenin Valueâ€™si daha Ã§ok alÄ±nÄ±r.\n",
    "\n",
    "Yani matematiksel olarak â€œdikkatiâ€ hesaplÄ±yoruz.\n",
    "\n",
    "Bu mekanizma PDFâ€™de gÃ¶rsellerle gÃ¶steriliyor.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Self-Attention: Her Token, Herkese Bakar\n",
    "\n",
    "Transformerâ€™daki en kritik ÅŸey: **Her kelime tÃ¼m diÄŸerlerine bakÄ±yor.**\n",
    "\n",
    "Bu yÃ¼zden uzun mesafeli iliÅŸkiler _mÃ¼kemmel_ yakalanÄ±yor.\n",
    "\n",
    "### Ã–rnek:\n",
    "\n",
    "â€œcat sat on the matâ€\n",
    "\n",
    "â€œsatâ€ kelimesi:\n",
    "\n",
    "- â€œcatâ€ ile Ã§ok iliÅŸkili â†’ yÃ¼ksek dikkat\n",
    "- â€œmatâ€ ile biraz iliÅŸkili\n",
    "- â€œtheâ€ ile Ã§ok az iliÅŸkili\n",
    "\n",
    "Bu hesaplamalar paralel yapÄ±lÄ±r.\n",
    "\n",
    "PDF'deki â€œAttentionâ€ gÃ¶rselleri bu iliÅŸkilerin nasÄ±l hesaplandÄ±ÄŸÄ±nÄ± gÃ¶steriyor.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Multi-Head Attention (MHA)\n",
    "\n",
    "PDFâ€™de bÃ¼yÃ¼k bir bÃ¶lÃ¼m Multi-Head Attentionâ€™a ayrÄ±lmÄ±ÅŸ.\n",
    "\n",
    "AnlamasÄ± kolay:\n",
    "\n",
    "Her Attention head **dilin baÅŸka bir boyutunu Ã¶ÄŸrenir**.\n",
    "\n",
    "- Bir head â†’ Ã¶zne-yÃ¼klem iliÅŸkisini Ã¶ÄŸrenebilir  \n",
    "- Bir head â†’ zaman iliÅŸkilerini Ã¶ÄŸrenebilir  \n",
    "- Bir head â†’ duygusal baÄŸlamÄ± Ã¶ÄŸrenebilir\n",
    "\n",
    "Her head farklÄ± bir bakÄ±ÅŸ aÃ§Ä±sÄ±dÄ±r.\n",
    "\n",
    "Sonra tÃ¼m headâ€™ler birleÅŸtirilir.\n",
    "\n",
    "Bu yÃ¼zden Transformer Ã§ok gÃ¼Ã§lÃ¼dÃ¼r.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Ama: Attention Pozisyon Bilmez\n",
    "\n",
    "Attention yalnÄ±zca iliÅŸkilere bakar; sÄ±rayÄ± bilmez.\n",
    "\n",
    "PDF de bunu net sÃ¶ylÃ¼yor:\n",
    "\n",
    "> â€œAttention does not consider the position information â€¦ positional encoding solutionâ€\n",
    "\n",
    "Bu yÃ¼zden Transformerâ€™a ekstra **Positional Encoding** eklenir.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Positional Encoding (PE)\n",
    "\n",
    "Transformer sÄ±rayÄ± bilmez, bu yÃ¼zden her token embeddingâ€™ine ek bir â€œpozisyon bilgisiâ€ eklenir.\n",
    "\n",
    "PDFâ€™de:\n",
    "\n",
    "- PEâ€™nin embedding ile toplandÄ±ÄŸÄ± gÃ¶steriliyor  \n",
    "- Modern modellerde RoPE (Rotary Position Embedding) kullanÄ±lÄ±yor\n",
    "\n",
    "MantÄ±ÄŸÄ±:  \n",
    "Her kelimenin embeddingâ€™ine, bulunduÄŸu konumu anlatan sinÃ¼sâ€“cosinÃ¼s dalgalarÄ± eklenir.\n",
    "\n",
    "SonuÃ§:  \n",
    "Transformere sÄ±ranÄ±n anlamÄ±nÄ± Ã¶ÄŸretmiÅŸ oluruz.\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Normalization\n",
    "\n",
    "PDFâ€™de LayerNorm / RMSNorm farklarÄ± anlatÄ±lÄ±yor.\n",
    "\n",
    "Neden normalizasyon var?\n",
    "\n",
    "Ã‡Ã¼nkÃ¼:\n",
    "\n",
    "- Katmanlar bÃ¼yÃ¼dÃ¼kÃ§e deÄŸerler patlayabilir  \n",
    "- Ya da Ã§ok kÃ¼Ã§Ã¼lÃ¼p kaybolabilir\n",
    "\n",
    "Normalization:\n",
    "\n",
    "- AÄŸÄ±rlÄ±klarÄ± dengeler  \n",
    "- EÄŸitim stabil olur  \n",
    "- Daha hÄ±zlÄ± convergence saÄŸlar\n",
    "\n",
    "GPT-4 / Llama gibi modeller RMSNorm kullanÄ±r.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Feed Forward Network (FFN)\n",
    "\n",
    "PDFâ€™de Encoder/Decoder ÅŸemasÄ±nda gÃ¶rÃ¼nÃ¼yor.\n",
    "\n",
    "Transformerâ€™daki FFN aslÄ±nda ÅŸudur:\n",
    "\n",
    "> â€œAttention Ã§Ä±ktÄ±larÄ±nÄ± daha anlamlÄ± bir vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼ren kÃ¼Ã§Ã¼k bir MLP.â€\n",
    "\n",
    "- Her token baÄŸÄ±msÄ±z olarak iÅŸlenir  \n",
    "- Non-linearity eklenir  \n",
    "- Modelin daha kompleks iliÅŸkileri Ã¶ÄŸrenmesini saÄŸlar\n",
    "\n",
    "Activation olarak genelde **GELU** veya **SwiGLU** kullanÄ±lÄ±r.\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Transformerâ€™Ä±n Genel Mimarisi (PDF ile uyumlu)\n",
    "\n",
    "PDFâ€™de Encoderâ€“Decoder ÅŸemasÄ± var.\n",
    "\n",
    "Ama modern modeller genelde decoder-only (GPT gibi).\n",
    "\n",
    "### Encoder-Decoder (T5, BART, MT modelleri)\n",
    "\n",
    "- Encoder: inputâ€™u anlamlandÄ±rÄ±r  \n",
    "- Decoder: output Ã¼retir  \n",
    "- Encoderâ€“decoder attention vardÄ±r  \n",
    "\n",
    "### Decoder-only (GPT, Llama)\n",
    "\n",
    "- YalnÄ±zca decoder bloÄŸu  \n",
    "- Masked self-attention kullanÄ±r  \n",
    "- Otoregresif Ã¼retim yapar\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Token NasÄ±l Ãœretiliyor? (Bir Ã¶rnek Ã¼zerinden anlatalÄ±m)\n",
    "\n",
    "Burada amaÃ§, modelin **sÄ±radaki kelimeyi** nasÄ±l seÃ§tiÄŸini anlamak.  \n",
    "Ã–rneÄŸimiz: **â€œBugÃ¼n hava Ã§ok â€¦â€**\n",
    "\n",
    "\n",
    "\n",
    "### 1. Model cÃ¼mleyi â€œdÃ¼ÅŸÃ¼nÃ¼râ€ â†’ bir vektÃ¶r Ã¼retir\n",
    "\n",
    "Model â€œBugÃ¼n hava Ã§okâ€ ifadesine bakÄ±p ÅŸu anlamÄ± Ã§Ä±karÄ±r: \n",
    "- Hava ile ilgili bir ÅŸey gelecek   \n",
    "- SÄ±fat gelecek (sÄ±cak, soÄŸuk, gÃ¼zelâ€¦)    \n",
    "- GÃ¼nlÃ¼k konuÅŸma    \n",
    "- BaÄŸlam olumlu/olumsuz olabilir    \n",
    "\n",
    "Bu dÃ¼ÅŸÃ¼nme â†’ **bir vektÃ¶rle** ifade edilir.   \n",
    "Diyelim ki vektÃ¶rÃ¼n boyutu **4096** olsun.    \n",
    "\n",
    "\n",
    "\n",
    "### 2. Bu vektÃ¶r â†’ sÃ¶zlÃ¼kteki her kelime iÃ§in skor Ã¼retir (logits)\n",
    "\n",
    "Modelin sÃ¶zlÃ¼ÄŸÃ¼nde diyelim ki **50.000 kelime** var.\n",
    "\n",
    "Modelin en sonunda, boyutu:\n",
    "\n",
    "> `4096 x 50.000`   Yani cÃ¼mlemizden Ã§Ä±kan vektÃ¶r ile modelin sÃ¶zlÃ¼ÄŸÃ¼ndeki kelimeler matris Ã§arpÄ±mÄ± yapÄ±lÄ±r\n",
    "\n",
    "> `hidden_state (4096)  Ã—  W (4096 x 50.000)  â†’  logits (50.000)`\n",
    "\n",
    "SonuÃ§:  \n",
    "SÃ¶zlÃ¼kteki **her bir kelime iÃ§in bir skor** (logit) ortaya Ã§Ä±kar.\n",
    "\n",
    "Ã–rneÄŸin:\n",
    "\n",
    "| Kelime   | Logit (ham skor) |\n",
    "|----------|------------------|\n",
    "| sÄ±cak    | 9.8              |\n",
    "| soÄŸuk    | 8.1              |\n",
    "| gÃ¼zel    | 7.5              |\n",
    "| yaÄŸmurlu | 4.2              |\n",
    "| kÄ±rmÄ±zÄ±  | 0.1              |\n",
    "| araba    | -3.5             |\n",
    "| ...      | ...              |\n",
    "\n",
    "\n",
    "\n",
    "### 3. Softmax uygulanÄ±r â†’ skorlar olasÄ±lÄ±ÄŸa dÃ¶nÃ¼ÅŸÃ¼r\n",
    "\n",
    "Logits deÄŸeri yÃ¼ksek olan kelimeler, softmax sonrasÄ± **daha yÃ¼ksek olasÄ±lÄ±ÄŸa** sahip olur.\n",
    "\n",
    "Ã–rneÄŸin:\n",
    "\n",
    "| Kelime   | OlasÄ±lÄ±k |\n",
    "|----------|----------|\n",
    "| sÄ±cak    | %45      |\n",
    "| soÄŸuk    | %30      |\n",
    "| gÃ¼zel    | %20      |\n",
    "| yaÄŸmurlu | %4       |\n",
    "| kÄ±rmÄ±zÄ±  | %0.2     |\n",
    "| araba    | %0.01    |\n",
    "\n",
    "ArtÄ±k modelin elinde:  \n",
    "**â€œBugÃ¼n hava Ã§ok â€¦â€ cÃ¼mlesini hangi kelimeyle tamamlama olasÄ±lÄ±ÄŸÄ±** var.\n",
    "\n",
    "\n",
    "\n",
    "### 4. OlasÄ±lÄ±klardan token seÃ§imi (sampling)\n",
    "\n",
    "Bu noktada seÃ§im nasÄ±l yapÄ±lÄ±rsa, modelin â€œyaratÄ±cÄ±lÄ±ÄŸÄ±â€ da ona gÃ¶re deÄŸiÅŸir:\n",
    "\n",
    "- **Top-k:**  \n",
    "  Sadece en yÃ¼ksek olasÄ±lÄ±klÄ± **k** kelime iÃ§inden seÃ§im yapÄ±lÄ±r.  \n",
    "  Ã–rneÄŸin k = 3 ise: `sÄ±cak, soÄŸuk, gÃ¼zel` arasÄ±ndan seÃ§im yapÄ±lÄ±r.\n",
    "\n",
    "- **Top-p (nucleus sampling):**  \n",
    "  OlasÄ±lÄ±klarÄ± toplayarak belli bir eÅŸik deÄŸere (Ã¶r. p = 0.9) ulaÅŸana kadar kelimeler seÃ§ilir.  \n",
    "  `%45 + %30 + %20 = %95` â†’ yine `sÄ±cak, soÄŸuk, gÃ¼zel` havuzuna dÃ¼ÅŸeriz.\n",
    "\n",
    "- **Temperature:**  \n",
    "  OlasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ±n keskinliÄŸini ayarlar.\n",
    "  - DÃ¼ÅŸÃ¼k sÄ±caklÄ±k (Ã¶rn. 0.2) â†’ DaÄŸÄ±lÄ±m keskinleÅŸir, **â€œsÄ±cakâ€ neredeyse garanti Ã§Ä±kar.**\n",
    "  - YÃ¼ksek sÄ±caklÄ±k (Ã¶rn. 1.2) â†’ DaÄŸÄ±lÄ±m dÃ¼zleÅŸir, **â€œgÃ¼zelâ€ veya daha az olasÄ± kelimeler** de seÃ§ilebilir.\n",
    "\n",
    "SeÃ§im sonrasÄ± model Ã¶rneÄŸin:\n",
    "\n",
    "> **â€œBugÃ¼n hava Ã§ok sÄ±cak.â€**\n",
    "\n",
    "ÅŸeklinde cÃ¼mleyi tamamlar.\n",
    "\n",
    "\n",
    "\n",
    "### 5. KÄ±sa Ã–zet\n",
    "\n",
    "1. Model, baÄŸlamdan bir **gizli vektÃ¶r** Ã¼retir.  \n",
    "2. Bu vektÃ¶r, Ã§Ä±kÄ±ÅŸ katmanÄ±ndaki **bÃ¼yÃ¼k matrisle Ã§arpÄ±lÄ±r** â†’ her kelime iÃ§in bir **logit** oluÅŸur.  \n",
    "3. **Softmax** ile logits â†’ **olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±na** Ã§evrilir.  \n",
    "4. **Top-k / Top-p / Temperature** gibi yÃ¶ntemlerle bu daÄŸÄ±lÄ±mdan bir **token (kelime)** seÃ§ilir.\n",
    "\n",
    "> Yani model aslÄ±nda her adÄ±mda:\n",
    "> **â€œTÃ¼m kelimeler iÃ§in olasÄ±lÄ±k hesapla â†’ birini seÃ§ â†’ cÃ¼mleye ekleâ€**\n",
    "> sÃ¼recini tekrarlar.\n",
    "\n",
    "---\n",
    "\n",
    "## 11) KV-Cache (PDFâ€™de uzun anlatÄ±m)\n",
    "\n",
    "PDF bunu Ã§ok net aÃ§Ä±klÄ±yor:\n",
    "\n",
    "> â€œLLM'lerde, tÃ¼m iliÅŸkiler her yeni Ã¼retilen token ile baÅŸtan hesaplanÄ±r, bu da gereksiz hesaplama maliyetine yol aÃ§ar. KV-cache ile, Ã¶nceki adÄ±mlarda hesaplanan iliÅŸkiler (Anahtar ve DeÄŸer Ã§iftleri) bellekte saklanÄ±r, bÃ¶ylece hesaplama maliyeti bÃ¼yÃ¼k Ã¶lÃ§Ã¼de azalÄ±r.â€\n",
    "\n",
    "MantÄ±ÄŸÄ±:\n",
    "\n",
    "Normalde:\n",
    "\n",
    "- Her adÄ±mda tÃ¼m attention yeniden yapÄ±lÄ±r â†’ Ã§ok yavaÅŸ\n",
    "\n",
    "KV-cache:\n",
    "\n",
    "- Eski tokenlarÄ±n K ve V deÄŸerleri saklanÄ±r  \n",
    "- Sadece yeni token iÃ§in hesap yapÄ±lÄ±r  \n",
    "- Muazzam hÄ±z kazancÄ± saÄŸlar\n",
    "\n",
    "Dezavantaj:\n",
    "\n",
    "- HafÄ±za kullanÄ±mÄ± artar  \n",
    "- Decode kÄ±smÄ± hÄ±zlÄ± ama memory yoÄŸun\n",
    "\n",
    "---\n",
    "\n",
    "## 11) Head YÃ¶ntemleri\n",
    "\n",
    "AÅŸaÄŸÄ±da MHA, MQA, GQA, MLA ve MoE yÃ¶ntemlerinin **Ã§ok basit ve anlaÅŸÄ±lÄ±r** bir Ã¶zetini bulacaksÄ±n.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ 1) MHA â€“ Multi Head Attention (Klasik yÃ¶ntem)\n",
    "\n",
    "**Nedir?**  \n",
    "- Birden fazla head vardÄ±r (8, 16, 32â€¦)  \n",
    "- Her head kendi **Q, K, V** matrislerine sahiptir\n",
    "\n",
    "**AvantajlarÄ±:**  \n",
    "- En kaliteli dikkat mekanizmasÄ±  \n",
    "- YÃ¼ksek temsil gÃ¼cÃ¼\n",
    "\n",
    "**DezavantajlarÄ±:**  \n",
    "- Bellek tÃ¼ketimi yÃ¼ksek  \n",
    "- K ve V matrisleri her head iÃ§in tekrar edildiÄŸi iÃ§in pahalÄ±\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ 2) MQA â€“ Multi Query Attention (HÄ±z odaklÄ± modern yÃ¶ntem)\n",
    "\n",
    "**Nedir?**  \n",
    "- Q = Ã§oklu  \n",
    "- K = **tek**  \n",
    "- V = **tek**  \n",
    "â†’ TÃ¼m headâ€™ler aynÄ± K ve Vâ€™yi kullanÄ±r\n",
    "\n",
    "**AvantajlarÄ±:**  \n",
    "- Bellek kullanÄ±mÄ± bÃ¼yÃ¼k oranda dÃ¼ÅŸer  \n",
    "- Uzun sequenceâ€™larda Ã§ok hÄ±zlÄ±  \n",
    "- Pek Ã§ok modern LLM (Gemini, PaLMâ€¦) bunu kullanÄ±r\n",
    "\n",
    "**DezavantajlarÄ±:**  \n",
    "- MHA kadar esnek deÄŸil  \n",
    "- Temsil gÃ¼cÃ¼ bazÄ± durumlarda biraz dÃ¼ÅŸebilir (pratikte sorun olmaz)\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ 3) GQA â€“ Grouped Query Attention (Orta yol)\n",
    "\n",
    "**Nedir?**  \n",
    "- Headâ€™ler gruplara ayrÄ±lÄ±r  \n",
    "- Her grup kendi K ve Vâ€™sine sahiptir  \n",
    "â†’ MHA kadar Ã§ok K/V yok, MQA kadar az da deÄŸil\n",
    "\n",
    "**AvantajlarÄ±:**  \n",
    "- Kalite â‰ˆ MHA  \n",
    "- HÄ±z & bellek â‰ˆ MQA  \n",
    "- Llama 3, Qwen, Falcon gibi modellerde kullanÄ±lÄ±r\n",
    "\n",
    "**Ã–zet:**  \n",
    "**MHA kadar kaliteli + MQA kadar ucuz** â†’ mÃ¼kemmel denge\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ 4) MLA â€“ Multi-Head Latent Attention (DeepSeek tarzÄ±)\n",
    "\n",
    "**Nedir?**  \n",
    "- Q, K, V Ã¶nce daha kÃ¼Ã§Ã¼k bir **latent** alana projelenir  \n",
    "- Attention bu daha kompakt alanda yapÄ±lÄ±r\n",
    "\n",
    "**AvantajlarÄ±:**  \n",
    "- Daha hÄ±zlÄ±  \n",
    "- Daha az bellek  \n",
    "- Yine de MHA performansÄ±na yakÄ±n kalÄ±r\n",
    "\n",
    "**Neden kullanÄ±lÄ±r?**  \n",
    "- AynÄ± kaliteyi daha dÃ¼ÅŸÃ¼k maliyetle yapmak iÃ§in  \n",
    "- Yeni nesil modeller (DeepSeek-V3 vb.) bu yÃ¶ntemi kullanÄ±yor\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ 5) MoE â€“ Mixture of Experts (En geliÅŸmiÅŸ yaklaÅŸÄ±m)\n",
    "\n",
    "**Nedir?**  \n",
    "- Ä°Ã§eride **birÃ§ok uzman (expert) FFN** bulunur  \n",
    "- Router, her tokenâ€™Ä±n hangi uzmanlara gitmesi gerektiÄŸini seÃ§er  \n",
    "- Genelde uzmanlarÄ±n yalnÄ±zca **%5â€“10â€™u** aktif olur\n",
    "\n",
    "**AvantajlarÄ±:**  \n",
    "- Ã‡ok bÃ¼yÃ¼k kapasite (Ã§ok parametre)  \n",
    "- DÃ¼ÅŸÃ¼k hesaplama maliyeti  \n",
    "- GÃ¼Ã§lÃ¼ ama verimli\n",
    "\n",
    "**DezavantajlarÄ±:**  \n",
    "- EÄŸitim zordur  \n",
    "- Router dengesi kritik  \n",
    "- UzmanlarÄ±n yÃ¼k daÄŸÄ±lÄ±mÄ± bozulursa model verimsiz Ã¶ÄŸrenir\n",
    "\n",
    "Modern LLMâ€™lerde (DeepSeek, Claude, Gemini, Qwenâ€¦) yaygÄ±n ÅŸekilde kullanÄ±lÄ±r.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ Hepsinin AmacÄ± KÄ±saca\n",
    "\n",
    "| YÃ¶ntem | AmaÃ§ |\n",
    "|--------|------|\n",
    "| **MHA** | En gÃ¼Ã§lÃ¼, en klasik dikkat yÃ¶ntemi |\n",
    "| **MQA** | DÃ¼ÅŸÃ¼k bellek + yÃ¼ksek hÄ±z |\n",
    "| **GQA** | Hem kaliteli hem verimli â€” iyi denge |\n",
    "| **MLA** | SÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ, optimize edilmiÅŸ attention |\n",
    "| **MoE** | Devasa kapasite + dÃ¼ÅŸÃ¼k hesaplama |\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“Œ Ultra KÄ±sa Ã–zet\n",
    "\n",
    "- **MHA:** En kaliteli ama maliyetli  \n",
    "- **MQA:** En hÄ±zlÄ± ve en ucuz  \n",
    "- **GQA:** MHA kalitesi + MQA hÄ±zÄ±  \n",
    "- **MLA:** Yeni nesil, sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ attention  \n",
    "- **MoE:** â€œHer token iÃ§in sadece doÄŸru uzman Ã§alÄ±ÅŸsÄ±nâ€ yaklaÅŸÄ±mÄ±  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
